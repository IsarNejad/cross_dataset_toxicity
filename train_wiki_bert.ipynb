{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXgA7c3O_ioG"
   },
   "source": [
    "#Preparing the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYhR6Q5tE-zD"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzvyd7UXRxNv"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xr92-n7xK59t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEL-8VHTE3LN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6R8sZn535lQO"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSjk3innp6Pk"
   },
   "outputs": [],
   "source": [
    "cd \"Your_Current_Directory\" #/content/drive/My Drive/Colab_Notebooks/toxicity/wiki-lda-share/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aowg5VpQCvI8"
   },
   "source": [
    "#Preparing the sub-category of Wiki-dataset for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbbWZpkq5mF8"
   },
   "outputs": [],
   "source": [
    "topic_categories={1:[0,1],\n",
    "                  2:[2,7,8,9,12,14,16],\n",
    "                  3:[3,4,5,6,10,11,13,15,17,18,19]}\n",
    "\n",
    "comments_org = pd.read_csv('wiki_lda_topics_lda_probabilities.csv')\n",
    "#comments_1 = comments_org[comments_org['wiki_topic'].isin(topic_categories[0])]\n",
    "#comments_2 = comments_org[comments_org['wiki_topic'].isin(topic_categories[1]+topic_categories[3])][comments_org['toxicity']==0 ].sample(random_state = 100, n = 20000)\n",
    "#comments_3 = comments_org[comments_org['wiki_topic'].isin(topic_categories[1]+topic_categories[3])][comments_org['toxicity']==1 ]\n",
    "#comments =  pd.concat([comments_1,comments_2, comments_3])\n",
    "comments = comments_org\n",
    "len(comments[comments['toxicity']==0]), len(comments[comments['toxicity']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juBPLLvFDASy"
   },
   "source": [
    "#Functions for data processing, training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRbxTDOWVWLQ"
   },
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GUWg8au58FH"
   },
   "outputs": [],
   "source": [
    "def load_dataset(features):\n",
    "  all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "  all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "  all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "  all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "  return TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGW-Hcbn6Bfy"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer):\n",
    "  max_length = 128\n",
    "  labels = [example.label for example in examples]\n",
    "  batch_encoding = tokenizer.batch_encode_plus(\n",
    "         [(example.text_a, example.text_b) for example in examples], max_length=max_length, pad_to_max_length=True,truncation=True,\n",
    "    )\n",
    "  features = []\n",
    "  for i in range(len(examples)):\n",
    "      inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "\n",
    "      feature = InputFeatures(**inputs, label=labels[i])\n",
    "      features.append(feature)\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yp4EHBbj6LeJ"
   },
   "outputs": [],
   "source": [
    "def get_dataloader_from_dataframe(dataframe, mode, batch_size = 16, tokenizer = tokenizer  ):\n",
    "  examples = []\n",
    "  for row in dataframe.iterrows():\n",
    "    examples.append(InputExample(guid = row[0], text_a = row[1]['comment'],label=int(row[1]['toxicity'])))\n",
    "  features = convert_examples_to_features(examples, tokenizer)\n",
    "  dataset = load_dataset(features)\n",
    "  if mode == 'train':\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "  else: \n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=False)\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y66kcuEX6bv_"
   },
   "outputs": [],
   "source": [
    "class ToxicityClassifier:\n",
    "  def __init__(self, train_dataloader = None, num_labels = 2):\n",
    "    self.train_dataloader = train_dataloader\n",
    "    \n",
    "    self.num_labels = num_labels\n",
    "    if torch.cuda.is_available():\n",
    "          self.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        self.device = \"cpu\" \n",
    "    \n",
    "    model_class = BertForSequenceClassification \n",
    "    self.model = model_class.from_pretrained('bert-base-uncased')\n",
    "    self.gradient_accumulation_steps = 1\n",
    "    self.learning_rate = 2e-5\n",
    "    self.model.to(self.device)\n",
    "    self.precisions = None\n",
    "\n",
    "  def train(self, train_dataloader=None,num_train_epochs = 2 ):\n",
    "    if train_dataloader== None:\n",
    "      train_dataloader = self.train_dataloader\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    param_optimizer = list(self.model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "                                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                                    ]\n",
    "    t_total = len(train_dataloader) // self.gradient_accumulation_steps * num_train_epochs\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=math.ceil(t_total * 0.06), num_training_steps=t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    \n",
    "    self.model.zero_grad()\n",
    "    self.model.train()       \n",
    "    for epoch_num in trange(int(num_train_epochs)):\n",
    "        print('epoch #', epoch_num)\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples = 0\n",
    "        nb_tr_steps = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "\n",
    "            inputs = {\n",
    "                \"input_ids\":      batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "                \"labels\":         batch[3]\n",
    "            }\n",
    "            output = self.model(**inputs)\n",
    "          \n",
    "            \n",
    "            loss = output[0]\n",
    "            \n",
    "            if self.gradient_accumulation_steps > 1:\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % self.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                self.model.zero_grad()\n",
    "                global_step += 1\n",
    "            \n",
    "        print('train_loss = ',tr_loss / nb_tr_steps )\n",
    "        #f1s, accuracy, eval_loss =self.test_and_eval(mode = 'eval')\n",
    "        #print('eval_loss = ',eval_loss)\n",
    "        #print('eval_f1 = ', f1s)\n",
    "\n",
    "    return global_step, tr_loss / nb_tr_steps \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1z3tE8Z6trB"
   },
   "outputs": [],
   "source": [
    "def test_and_eval(model,test_dataloader ):\n",
    "   \n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "          device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = \"cpu\" \n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\":      batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "                \"labels\":         batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(\n",
    "                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    #print('\\n\\n', classification_report(out_label_ids, preds))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btkNnXXuDurK"
   },
   "source": [
    "#Train the classifier and save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMk-eaKP6GNK"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = get_dataloader_from_dataframe(comments, mode='train', batch_size = 16, tokenizer = tokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSoqEhps6c5f"
   },
   "outputs": [],
   "source": [
    "wiki_toxic = ToxicityClassifier(train_dataloader = train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeq46JeJ6fmV"
   },
   "outputs": [],
   "source": [
    "wiki_toxic.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbs4fNIt01zJ"
   },
   "outputs": [],
   "source": [
    "model_to_save = wiki_toxic.model.module if hasattr(wiki_toxic.model, 'module') else wiki_toxic.model  \n",
    "output_model_file = \"wiki_bert_2_epoch.bin\"\n",
    "torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XTFznYRPEByM"
   },
   "source": [
    "#Test on Waseem-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VsgthcA6xNn"
   },
   "outputs": [],
   "source": [
    "waseem_df = pd.read_csv('waseem_wiki_lda_topics_lda_probabilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykGzHMWl6x_B"
   },
   "outputs": [],
   "source": [
    "test_batch_size = 128\n",
    "waseem_dataloader = get_dataloader_from_dataframe(waseem_df, mode='eval', batch_size = test_batch_size, tokenizer = tokenizer  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UMhSQnJ6z9_"
   },
   "outputs": [],
   "source": [
    "waseem_preds = test_and_eval(wiki_toxic.model,test_dataloader = waseem_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzGgpMyy62aI"
   },
   "outputs": [],
   "source": [
    "waseem_df['wiki_toxicity'] = waseem_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMt2Cm7067GS"
   },
   "outputs": [],
   "source": [
    "sexism= waseem_df[waseem_df['Annotation']=='sexism']#[waseem_df['wiki_topic'].isin(topic_categories[1])]\n",
    "accuracy_score(sexism['toxicity'].tolist(), sexism['wiki_toxicity'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-_g4l3e6_Hq"
   },
   "outputs": [],
   "source": [
    "racism= waseem_df[waseem_df['Annotation']=='racism']#[waseem_df['wiki_topic'].isin(topic_categories[1])]\n",
    "accuracy_score(racism['toxicity'].tolist(), racism['wiki_toxicity'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwiXk1cDEOJd"
   },
   "source": [
    "#Test on Founta-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYpfSjHd7C0a"
   },
   "outputs": [],
   "source": [
    "Founta_df = pd.read_csv('Founta_wiki_lda_topics_lda_probabilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGrNZAyz7D1K"
   },
   "outputs": [],
   "source": [
    "Founta_dataloader = get_dataloader_from_dataframe(Founta_df, mode='eval', batch_size = test_batch_size, tokenizer = tokenizer  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DV9eSDH7GBi"
   },
   "outputs": [],
   "source": [
    "Founta_preds = test_and_eval(wiki_toxic.model,test_dataloader = Founta_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sfsGRHg7IGk"
   },
   "outputs": [],
   "source": [
    "Founta_df['wiki_toxicity'] = Founta_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ojyq78iJ7LKc"
   },
   "outputs": [],
   "source": [
    "abusive= Founta_df[Founta_df['label']=='abusive']#[waseem_df['wiki_topic'].isin(topic_categories[1])]\n",
    "accuracy_score(abusive['toxicity'].tolist(), abusive['wiki_toxicity'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52XH89yF7NJL"
   },
   "outputs": [],
   "source": [
    "hateful= Founta_df[Founta_df['label']=='hateful']#[waseem_df['wiki_topic'].isin(topic_categories[1])]\n",
    "accuracy_score(hateful['toxicity'].tolist(), hateful['wiki_toxicity'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j13In7kK7O9k"
   },
   "outputs": [],
   "source": [
    "normal= Founta_df[Founta_df['label']=='normal']#[waseem_df['wiki_topic'].isin(topic_categories[1])]\n",
    "accuracy_score(normal['toxicity'].tolist(), normal['wiki_toxicity'].tolist())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Train_wiki_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
